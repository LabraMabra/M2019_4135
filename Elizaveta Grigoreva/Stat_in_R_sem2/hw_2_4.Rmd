---
title: "hw_2_4"
author: "Grigoreva Elizaveta"
date: "6/8/2020"
output: pdf_document
---

Take the same data as for other classifications methods. Split it intro train and test 50/50. Apply random forest. Validate the result. Try to tune.

```{r}
#Load the same dataset
library(randomForest)
library(MASS)
library(ggplot2)
library(mlbench)
library(gbm)
```

```{r}
#Load data
data("BostonHousing")
Boston <-  BostonHousing
summary(Boston)
```
I will going to use variable ′medv′ as the Response variable, which is the Median Housing Value.
I will use all the predictors in the dataset.
```{r}
#Split our dataset to 50/50
set.seed(42)
train <-  sample(1:nrow(Boston), nrow(Boston)/2)
#Create two sets- valid and training
trainingset <- Boston[train,]
validset <- Boston[-train,]

```
The above Mean Squared Error and Variance explained are calculated using Out of Bag Error Estimation. The number
of randomly selected variables is 4
```{r}
#Build the random forest model,this command builds many regression trees 

boston_rf <- randomForest(medv ~ ., data=Boston, subset=train)
boston_rf
```
Plotting the Error vs Number of Trees Graph.
This plot shows the Error and the Number of Trees.We can easily notice that how the Error is dropping as we keep on adding more and more trees and average them.
```{r}
plot(boston_rf)
```
The above Random Forest model chose Randomly 4 variables to be considered at each split. We could now try all possible 13 predictors which can be found at each split. Our mtry varuiable is randomly chosen at each split 

```{r}
oob.err=double(13)
test.err=double(13)

for(mtry in 1:13) 
{
  rf=randomForest(medv ~ . , data = Boston , subset = train,mtry=mtry,ntree=400) 
  oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
  pred<-predict(rf,Boston[-train,]) #Predictions on Test Set for each Tree
  test.err[mtry]= with(Boston[-train,], mean( (medv - pred)^2)) #Mean Squared Test Error
  
  cat(mtry," ") #printing the output to the console
  
}
```
Test error
```{r}
test.err
```
Out of bag error estimation
```{r}
oob.err
```

We can see errpr and number of trees
```{r}
tree.boston= tree(medv ~ ., Boston, subset = train, mindev=.0001)

plot(tree.boston, type="u")
text(tree.boston,pretty = 0, cex=0.5)
```

```{r}
cv.boston = cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type="b")
prune.boston = prune.tree(tree.boston,best=6)
plot(prune.boston,type="u")
text(prune.boston,pretty=0)
```
We can do it on train data and valid set
```{r}
mod <- randomForest(medv ~ ., data = trainingset, mtry = 13, importance = T)
plot(mod)
mod
est_rm <- predict(mod, newdata = validset)
plot(est_rm, validset$medv)
abline(0,1)
mean((est_rm - validset$medv)^2)
```
```{r}
#mtry is no of Variables randomly chosen at each split
for(mtry in 1:13) 
{
  rf=randomForest(medv ~ . , data = trainingset,mtry=mtry,ntree=400) 
  oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
  
  pred<-predict(rf,validset) #Predictions on Test Set for each Tree
  test.err[mtry]= with(validset, mean( (medv - pred)^2)) #Mean Squared Test Error
  
  cat(mtry," ")
  
}
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))
```
Calculate depth. Obtain a plot of the distribution of minimal depth for top ten variables according to mean minimal depth calculated using top trees
```{r}
min_depth_frame <- min_depth_distribution(mod)
plot_min_depth_distribution(min_depth_frame, mean_sample = "relevant_trees", k = 15)
```

```{r}
set.seed(3)
bag.boston=randomForest(medv~.,data=trainingset,mtry=4, importance=TRUE)
yhat.bag = predict(bag.boston,newdata=validset)
mean((yhat.bag- validset$medv)^2)
importance(bag.boston)
varImpPlot(bag.boston)
```

```{r}
set.seed(1)
boost.boston=gbm(medv~.,data=trainingset,distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.boston)
```
```{r}
boost.boston=gbm(medv~.,data=trainingset,distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.02,verbose=F)
yhat.boost=predict(boost.boston,newdata=validset,n.trees=5000)
mean((yhat.boost-validset$medv)^2)
```

```{r}
mean((yhat.boost-validset$medv)^2)
```

```{r}
n.trees = seq(from=100 ,to=10000, by=200) 
```

```{r}
#Generating a Prediction matrix for each Tree
predmatrix<-predict(boost.boston,Boston[-train,],n.trees = n.trees)
dim(predmatrix) #dimentions of the Prediction Matrix
```


```{r}
#Calculating The Mean squared Test Error
test.error<-with(Boston[-train,],apply( (predmatrix-medv)^2,2,mean))
head(test.error) #contains the Mean squared test error for each of the 100 trees averaged
```


```{r}
plot(n.trees , test.error , pch=19,col="blue",xlab="Number of Trees",ylab="Test Error", main = "Perfomance of Boosting on Test Set")
```

```{r}
#adding the RandomForests Minimum Error line trained on same data and similar parameters
plot(n.trees , test.error , pch=19,col="blue",xlab="Number of Trees",ylab="Test Error", main = "Perfomance of Boosting on Test Set")
abline(h = min(test.err),col="red") #test.err is the test error of a Random forest fitted on same data
legend("topright",c("Minimum Test error Line for Random Forests"),col="red",lty=1,lwd=1)
boost.boston=gbm(medv~.,data=trainingset,distribution="gaussian",n.trees=1000,interaction.depth=4,shrinkage=0.02,verbose=F)
yhat.boost=predict(boost.boston,newdata=validset,n.trees=1000)
mean((yhat.boost-validset$medv)^2)
```

```{r}
hist(Boston$medv, breaks = 50)
boston_sub <- data.table(Boston)
boston_sub[,medv_Cat:= cut(medv,c(0,quantile(boston_sub$medv, 0.35), max(boston_sub$medv)),labels=c("rich","normal"))]

boston_sub %>%
  ggplot(aes(medv, color = medv_Cat))+
  geom_histogram(fill="white", alpha=0.5)
boston_sub <- boston_sub[,-14]
sampsize <-  ceiling(0.6*nrow(boston_sub[-train]))
mtry <- floor(sqrt(ncol(boston_sub)))
set.seed(3)
ntrees <- 50
rf <- randomForest(medv_Cat ~ ., data=boston_sub,
                   subset = train,
                   ntree = ntrees,
                   mtry = mtry,
                   sampsize = sampsize,
                   importance = TRUE,
                   do.trace = ntrees/25)
set.seed(3)
rf <- randomForest(medv_Cat ~ ., data=boston_sub,
                   subset = train,
                   ntree = 30,
                   mtry = mtry,
                   sampsize = sampsize,
                   importance = TRUE)
rf
pred <- predict(rf, newdata = boston_sub[-train,])
mean(pred!=bos$medv_Cat[-train])
varImpPlot(rf)
```




























